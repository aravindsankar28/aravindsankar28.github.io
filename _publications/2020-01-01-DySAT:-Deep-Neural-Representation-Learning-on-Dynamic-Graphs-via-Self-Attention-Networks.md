---
title: "DySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-Attention Networks"
collection: publications
permalink: /publication/2020-01-01-DySAT:-Deep-Neural-Representation-Learning-on-Dynamic-Graphs-via-Self-Attention-Networks
type: conference
date: 2020-01-01
venue: 'International Conference on Web Search and Data Mining, WSDM 2020, Houston, TX, February 3-7, 2020'
paperurl: '../files/DySAT-WSDM2020.pdf'
authors: '<strong>Aravind Sankar</strong>, Yanhong Wu, Liang Gou, Wei Zhang, Hao Yang'
url: 'https://arxiv.org/pdf/1812.09430.pdf'
abstract: 'Learning node representations in graphs is important for many applications such as link prediction, node classification, and community
detection. Existing graph representation learning methods primarily target static graphs while many real-world graphs evolve over time. Complex time-varying graph structures make it challenging to learn informative node representations over time.

We present Dynamic Self-Attention Network (DySAT), a novel neural architecture that learns node representations to capture dynamic graph structural evolution. Specifically, DySAT computes node representations through joint self-attention along the two dimensions of structural neighborhood and temporal dynamics. Compared with state-of-the-art recurrent methods modeling graph evolution, dynamic self-attention is efficient, while achieving consistently superior performance. We conduct link prediction experiments on two graph types: communication networks and bipartite rating networks. Experimental results demonstrate significant performance gains for DySAT over several state-of-the-art graph embedding baselines, in both single and multi-step link prediction tasks. Furthermore, our ablation study validates the effectiveness of jointly modeling structural and temporal self-attention.'
code: https://github.com/aravindsankar28/DySAT
---
