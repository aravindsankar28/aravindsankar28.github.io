---
title: "Deep Neural Representation Learning on Dynamic Graphs via Self-Attention Networks"
collection: publications
permalink: /publication/2020-01-01-Deep-Neural-Representation-Learning-on-Dynamic-Graphs-via-Self-Attention-Networks
type: conference
date: 2020-01-01
venue: 'International Conference on Web Search and Data Mining, WSDM 2020, Houston, TX, February 3-7, 2020'
paperurl: 'https://arxiv.org/pdf/1812.09430.pdf'
authors: '<strong>Aravind Sankar</strong>, Yanhong Wu, Liang Gou, Wei Zhang, Hao Yang'
url: 'https://arxiv.org/pdf/1812.09430.pdf'
abstract: 'Learning node representations in graphs is important for many applications such as link prediction, node classification, and community detection. Previous graph representation learning methods mainly focus on static graphs while many real-world graphs are dynamic and evolve over time. This complex time-varying structures makes it challenging to learn good node representations over time. We present Dynamic Self-Attention Network (DySAT), a novel neural architecture that learns node representations to capture dynamic graph structural evolution. Specifically, DySAT computes node representations by jointly employing self-attention layers along two dimensions: structural neighborhood and temporal dynamics. Compared with state-of-the-art recurrent methods modeling graph evolution, dynamic self-attention is efficient, while achieving consistently superior performance. We conduct link prediction experiments on two graph types: communication networks and bipartite rating networks. Our results show that DySAT has significant performance gains over several state-of-the-art graph embedding baselines, in both single and multi-step link prediction tasks. We also conduct an ablation study that shows the effectiveness of both the structural and temporal self-attentional layers.'
code: https://github.com/aravindsankar28/DySAT
---
